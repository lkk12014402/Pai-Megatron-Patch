nohup: ignoring input
+ LAUNCHER_TYPE=mpirun
+ DATA_DIR=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged
+ DATA_CACHE_DIR=
+ DATA_FILE_PREFIX=tokenized_text_document
+ TOKENIZER_MODEL=/lkk/Llama-3.1-8B/original/tokenizer.model
+ TRANSFORMER_IMPL=transformer_engine
+ NUM_NODES=1
+ DP=4
+ TP=2
+ PP=1
+ MICRO_BATCH_SIZE=1
+ EXIT_INTERVAL=0
+ OUTPUT_DIR=
+ OUTPUT_DIR_PREFIX=.
+ CHECKPOINT_SAVE=1
+ SAVE_INTERVAL=2000
+ DIST_CKPT_FORMAT=torch_dist
+ USE_DISTRIBUTED_OPTIMIZER=1
+ USE_DIST_CKPT=0
+ LOAD_DIR=
+ CHECKPOINTS_DIR=
+ VERIFY_CKPT=1
+ TENSORBOARD_DIR=
+ KILL_SWITCH_FILE=
+ HOSTSFILE=
+ CKP_ACT=2
+ RECOMPUTE_NUM_LAYERS=1
+ LOG_INTERVAL=10
+ LLAMA_VER=3.2
+ LLAMA_MODEL_SIZE=1
+ DEVICES_PER_NODE=8
+ SEQ_PARALLEL=1
+ OPTIMIZER=fusedadamw
+ DROPOUT=0.0
+ EVAL_ITERS=100
+ EVAL_INTERVAL=1000
+ USE_FUSED_SDPA=1
+ USE_FUSED_SDPA_WITH_RECOMPUTE=0
+ USE_FAST_SOFTMAX=1
+ USE_FUSED_RMSNORM=1
+ PROFILE_TYPE=
+ PROFILE_STEP_START=3
+ PROFILE_STEP_END=4
+ PROFILE_RANKS=0
+ REDIRECT_LOGS=0
+ DETERMINISTIC_MODE=1
+ FP8=0
+ FP8_FORMAT=hybrid
+ FP8_MARGIN=0
+ FP8_AMAX_COMPUTE_ALGO=max
+ USE_TORCH_COMPILE=0
+ USE_LAZY_MODE=1
+ SKIP_TRAIN=0
+ NUM_WORKERS=2
+ FP8_COVERAGE='mlp_row_parallel=False attention=False'
+ [[ -z '' ]]
+++ dirname /lkk/Megatron-LM/examples/llama/pretrain_llama.sh
++ realpath /lkk/Megatron-LM/examples/llama/../../
+ MEGATRON_LM_ROOT=/lkk/Megatron-LM
+ [[ 8 -ne 8 ]]
+ [[ transformer_engine = \l\o\c\a\l ]]
+ [[ 1 = 1 ]]
+ [[ 0 = 1 ]]
+ [[ 3.2 = \1 ]]
+ [[ 3.2 = \2 ]]
+ [[ 3.2 = \3\.\1 ]]
+ [[ 3.2 = \3\.\2 ]]
+ TOKENIZER_TYPE=Llama3Tokenizer
+ GLOBAL_BATCH_SIZE=2048
+ MAX_SEQ_LEN=4096
+ TRAIN_ITERS=937500
+ ADAM_EPS=1e-5
+ LR_WARMUP_ITERS=8000
+ ROTARY_BASE=500000
+ [[ 1 = \1 ]]
+ HIDDEN_SIZE=2048
+ NUM_HEADS=32
+ NUM_QUERY_GROUPS=8
+ NUM_LAYERS=16
+ FFN_HIDDEN_SIZE=8192
+ LR=4e-4
+ MIN_LR=3e-6
+ [[ 0 -ne 0 ]]
+ SRC_PATH=/lkk/Megatron-LM/pretrain_gpt.py
+ DATA_PATH=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document
+ [[ -z /lkk/Llama-3.1-8B/original/tokenizer.model ]]
+ NUM_DEVICES=8
++ date +%Y%m%d_%H%M
+ RUNTIME=20250107_0652
+ [[ -z '' ]]
+ EXP_NAME=default
+ [[ -z '' ]]
+ data_type=bf16
+ [[ 0 -eq 1 ]]
+ OUTPUT_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652
+ [[ -z '' ]]
+ CHECKPOINTS_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints
+ [[ -z '' ]]
+ LOAD_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints
+ [[ -z '' ]]
+ TENSORBOARD_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard
+ [[ 1 -ne 1 ]]
+ PT_HPU_GPU_MIGRATION=1
+ PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1
+ CMD=
+ [[ mpirun = \m\p\i\r\u\n ]]
+ CMD=' mpirun'
+ CMD=' mpirun --allow-run-as-root'
+ CMD=' mpirun --allow-run-as-root -n 8'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1'
+ [[ 1 -ne 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345'
+ '[' 1 = 0 ']'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2     '
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel'
+ [[ 1 = \1 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader'
+ [[ 0 -eq 1 ]]
+ [[ 2 -eq 1 ]]
+ [[ 2 -eq 2 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective'
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer'
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode'
+ [[ transformer_engine = \t\r\a\n\s\f\o\r\m\e\r\_\e\n\g\i\n\e ]]
+ [[ 0 -eq 1 ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --save-interval 2000'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist'
+ [[ 0 -eq 1 ]]
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA'
+ [[ Llama3Tokenizer = \G\P\T\S\e\n\t\e\n\c\e\P\i\e\c\e\T\o\k\e\n\i\z\e\r ]]
+ [[ Llama3Tokenizer = \L\l\a\m\a\3\T\o\k\e\n\i\z\e\r ]]
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer'
+ CMD=' mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer --tokenizer-model /lkk/Llama-3.1-8B/original/tokenizer.model'
+ [[ -n '' ]]
+ [[ 0 -eq 1 ]]
+ mpirun --allow-run-as-root -n 8 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345 python /lkk/Megatron-LM/pretrain_gpt.py --transformer-impl transformer_engine --tensor-model-parallel-size 2 --pipeline-model-parallel-size 1 --distributed-backend nccl --seq-length 4096 --num-layers 16 --hidden-size 2048 --num-attention-heads 32 --group-query-attention --num-query-groups 8 --ffn-hidden-size 8192 --position-embedding-type rope --rotary-base 500000 --max-position-embeddings 4096 --normalization RMSNorm --swiglu --untie-embeddings-and-output-weights --attention-dropout 0.0 --hidden-dropout 0.0 --weight-decay 1e-1 --clip-grad 1.0 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-5 --lr 4e-4 --lr-decay-style cosine --lr-warmup-iters 8000 --min-lr 3e-6 --use-torch-compile=0 --use-fused-sdpa-with-recompute 0 --use-fused-sdpa 1 --use-fused-rmsnorm 1 --use-fast-softmax 1 --micro-batch-size 1 --global-batch-size 2048 --train-iters 937500 --log-interval 10 --log-throughput --disable-bias-linear --optimizer fusedadamw --no-gradient-accumulation-fusion --no-masked-softmax-fusion --use-mcore-models --bf16 --exit-interval 0 --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard --log-validation-ppl-to-tensorboard --log-batch-size-to-tensorboard --log-timers-to-tensorboard --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --eval-interval 1000 --eval-iters 100 --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document --num-workers 2 --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer --tokenizer-model /lkk/Llama-3.1-8B/original/tokenizer.model
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056374480 KB
------------------------------------------------------------------------------
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
using world size: 8, data-parallel size: 4, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama3Tokenizer
WARNING: Please specify --split when using --data-path. Using legacy default value of "969, 30, 1"
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
is_gaudi3()=False
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-05
  add_bias_linear ................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_norm_post_sub_block ....................... False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.0
  attention_per_layer_logging ..................... False
  attention_softmax_in_fp32 ....................... False
  attention_z_loss_coeff .......................... 0.0
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. False
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 4
  data_path ....................................... ['/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  defer_embedding_wgrad_compute ................... False
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  deprecated_use_mcore_models ..................... True
  deterministic_mode .............................. True
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... True
  encoder_num_layers .............................. 16
  encoder_seq_length .............................. 4096
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 100
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 0
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_amax_reduce ................................. False
  fp8_coverage .................................... {'mlp_row_parallel': True, 'attention': True}
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 2048
  gradient_accumulation_fusion .................... False
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 2048
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kill_switch_file ................................ None
  kv_channels ..................................... 64
  lazy_mpu_init ................................... None
  load ............................................ ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints
  load_strict ..................................... True
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0004
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 8000
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-06
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_capacity_bins_alignment ..................... 64
  moe_capacity_bins_exp_base ...................... 1.5
  moe_capacity_bins_num ........................... 0
  moe_capacity_bins_optimize_interval ............. 300
  moe_capacity_bins_optimize_max_group ............ 4
  moe_configured_bins ............................. None
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 8
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... fusedadamw
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  profile_type .................................... None
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 500000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints
  save_interval ................................... 2000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 4096
  sequence_parallel ............................... True
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  shuffle_each_epoch_separately ................... False
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /lkk/Llama-3.1-8B/original/tokenizer.model
  tokenizer_type .................................. Llama3Tokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 937500
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... True
  use_fast_softmax ................................ True
  use_flash_attn .................................. False
  use_fused_rmsnorm ............................... True
  use_fused_sdpa .................................. True
  use_fused_sdpa_with_recompute ................... False
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_torch_compile ............................... False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  verify_checkpoint ............................... True
  verify_checkpoint_model_type .................... LLAMA
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 8
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of micro-batches to constant 512
> building Llama3Tokenizer tokenizer ...
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
 > padded vocab (size: 128256) with 0 dummy tokens (new size: 128256)
> initializing torch distributed ...
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/lkk/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/lkk/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.240 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
time to initialize megatron (seconds): 30.991
[after megatron is initialized] datetime: 2025-01-07 06:52:14 
building GPT model ...
TransformerConfig(tensor_model_parallel_size=2, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, expert_model_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=True, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=16, hidden_size=2048, num_attention_heads=32, num_query_groups=8, ffn_hidden_size=8192, kv_channels=64, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, apply_norm_post_sub_block=False, layernorm_epsilon=1e-05, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7fbdc2be8820>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=False, test_mode=False, calculate_per_token_loss=False, attention_z_loss_coeff=0.0, init_method=<function init_method_normal.<locals>.init_ at 0x7fbd94c4bf40>, output_layer_init_method=<function scaled_init_method_normal.<locals>.init_ at 0x7fbd94512f80>, init_method_std=0.02, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=True, apply_rope_fusion=True, use_fused_rmsnorm=True, use_fused_sdpa=True, use_fused_sdpa_with_recompute=False, use_fast_softmax=True, recompute_granularity='selective', recompute_method=None, recompute_num_layers=None, distribute_saved_activations=False, fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, fp8_amax_reduce=False, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_pre_softmax=False, moe_grouped_gemm=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='allgather', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_capacity_bins_num=0, moe_capacity_bins_exp_base=1.5, moe_capacity_bins_optimize_interval=300, moe_capacity_bins_optimize_max_group=4, moe_capacity_bins_alignment=64, moe_configured_bins=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, enable_cuda_graph=False)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 749275136
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 749275136
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, use_distributed_optimizer=True, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (749275136 elements):
	module.decoder.layers.14.self_attention.linear_qkv.weight
	module.decoder.layers.12.self_attention.linear_proj.weight
	module.decoder.layers.8.pre_mlp_layernorm.weight
	module.decoder.layers.1.mlp.linear_fc2.weight
	module.decoder.layers.15.input_layernorm.weight
	module.decoder.layers.1.self_attention.linear_qkv.weight
	module.decoder.layers.11.mlp.linear_fc2.weight
	module.decoder.layers.7.input_layernorm.weight
	module.decoder.layers.4.mlp.linear_fc1.weight
	module.decoder.layers.5.pre_mlp_layernorm.weight
	module.decoder.layers.13.pre_mlp_layernorm.weight
	module.decoder.layers.5.mlp.linear_fc1.weight
	module.decoder.layers.12.input_layernorm.weight
	module.decoder.layers.7.mlp.linear_fc1.weight
	module.decoder.layers.3.self_attention.linear_qkv.weight
	module.decoder.layers.15.mlp.linear_fc1.weight
	module.decoder.layers.13.mlp.linear_fc2.weight
	module.decoder.layers.11.self_attention.linear_qkv.weight
	module.decoder.layers.8.self_attention.linear_proj.weight
	module.decoder.layers.6.mlp.linear_fc1.weight
	module.decoder.layers.4.input_layernorm.weight
	module.decoder.layers.2.pre_mlp_layernorm.weight
	module.decoder.layers.14.self_attention.linear_proj.weight
	module.decoder.layers.10.pre_mlp_layernorm.weight
	module.decoder.layers.9.input_layernorm.weight
	module.decoder.layers.0.self_attention.linear_qkv.weight
	module.decoder.final_layernorm.weight
	module.decoder.layers.10.self_attention.linear_qkv.weight
	module.decoder.layers.0.mlp.linear_fc2.weight
	module.output_layer.weight
	module.decoder.layers.15.pre_mlp_layernorm.weight
	module.decoder.layers.13.self_attention.linear_qkv.weight
	module.decoder.layers.0.pre_mlp_layernorm.weight
	module.decoder.layers.7.pre_mlp_layernorm.weight
	module.decoder.layers.2.mlp.linear_fc2.weight
	module.decoder.layers.0.self_attention.linear_proj.weight
	module.decoder.layers.6.input_layernorm.weight
	module.decoder.layers.15.mlp.linear_fc2.weight
	module.decoder.layers.10.mlp.linear_fc1.weight
	module.decoder.layers.8.mlp.linear_fc2.weight
	module.decoder.layers.6.mlp.linear_fc2.weight
	module.decoder.layers.2.mlp.linear_fc1.weight
	module.decoder.layers.5.mlp.linear_fc2.weight
	module.decoder.layers.14.input_layernorm.weight
	module.decoder.layers.7.mlp.linear_fc2.weight
	module.decoder.layers.3.self_attention.linear_proj.weight
	module.decoder.layers.12.pre_mlp_layernorm.weight
	module.decoder.layers.2.self_attention.linear_proj.weight
	module.decoder.layers.8.mlp.linear_fc1.weight
	module.decoder.layers.4.pre_mlp_layernorm.weight
	module.decoder.layers.2.self_attention.linear_qkv.weight
	module.decoder.layers.1.self_attention.linear_proj.weight
	module.decoder.layers.11.self_attention.linear_proj.weight
	module.decoder.layers.10.self_attention.linear_proj.weight
	module.decoder.layers.4.self_attention.linear_qkv.weight
	module.decoder.layers.3.input_layernorm.weight
	module.decoder.layers.14.mlp.linear_fc1.weight
	module.decoder.layers.11.input_layernorm.weight
	module.decoder.layers.9.mlp.linear_fc1.weight
	module.decoder.layers.9.pre_mlp_layernorm.weight
	module.decoder.layers.9.self_attention.linear_qkv.weight
	module.decoder.layers.7.self_attention.linear_qkv.weight
	module.decoder.layers.5.self_attention.linear_qkv.weight
	module.embedding.word_embeddings.weight
	module.decoder.layers.15.self_attention.linear_qkv.weight
	module.decoder.layers.13.self_attention.linear_proj.weight
	module.decoder.layers.4.self_attention.linear_proj.weight
	module.decoder.layers.1.pre_mlp_layernorm.weight
	module.decoder.layers.3.mlp.linear_fc2.weight
	module.decoder.layers.1.input_layernorm.weight
	module.decoder.layers.12.self_attention.linear_qkv.weight
	module.decoder.layers.8.input_layernorm.weight
	module.decoder.layers.6.pre_mlp_layernorm.weight
	module.decoder.layers.4.mlp.linear_fc2.weight
	module.decoder.layers.14.pre_mlp_layernorm.weight
	module.decoder.layers.3.mlp.linear_fc1.weight
	module.decoder.layers.1.mlp.linear_fc1.weight
	module.decoder.layers.0.input_layernorm.weight
	module.decoder.layers.14.mlp.linear_fc2.weight
	module.decoder.layers.13.input_layernorm.weight
	module.decoder.layers.12.mlp.linear_fc1.weight
	module.decoder.layers.9.mlp.linear_fc2.weight
	module.decoder.layers.5.input_layernorm.weight
	module.decoder.layers.6.self_attention.linear_proj.weight
	module.decoder.layers.3.pre_mlp_layernorm.weight
	module.decoder.layers.15.self_attention.linear_proj.weight
	module.decoder.layers.11.mlp.linear_fc1.weight
	module.decoder.layers.10.mlp.linear_fc2.weight
	module.decoder.layers.5.self_attention.linear_proj.weight
	module.decoder.layers.12.mlp.linear_fc2.weight
	module.decoder.layers.11.pre_mlp_layernorm.weight
	module.decoder.layers.7.self_attention.linear_proj.weight
	module.decoder.layers.10.input_layernorm.weight
	module.decoder.layers.9.self_attention.linear_proj.weight
	module.decoder.layers.6.self_attention.linear_qkv.weight
	module.decoder.layers.13.mlp.linear_fc1.weight
	module.decoder.layers.8.self_attention.linear_qkv.weight
	module.decoder.layers.2.input_layernorm.weight
	module.decoder.layers.0.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='fusedadamw', lr=0.0004, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7fbdc1300a30>)
> learning rate decay style: cosine
WARNING: could not find the metadata file ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D4_T2_P1_devices8_20250107_0652/checkpoints/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
(min, max) time across ranks (ms):
    load-checkpoint ................................: (0.63, 57.41)
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-01-07 06:52:15 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1920000000
    validation: 192102400
    test:       204800
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.969), (0.969, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(1920000000, 192102400, 204800), and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=(['/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document'], None), blend_per_split=[None, None, None], split='969, 30, 1', split_matrix=[(0, 0.969), (0.969, 0.999), (0.999, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer.create_llama3_tokenizer.<locals>._Llama3Tokenizer object at 0x7fbd94d79690>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None, shuffle_each_epoch_separately=False)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 91269129
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 91269129
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1931285131
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 93
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 192511817
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 306
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 205519
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 10
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2025-01-07 07:05:03 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (694.87, 716.52)
    train/valid/test-data-iterators-setup ..........: (767288.33, 767914.43)
[before the start of training step] datetime: 2025-01-07 07:05:03 
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
[Rank 1] (after 10 iterations) memory (MB) | allocated: 7453.8538818359375 | max allocated: 13231.092163085938 | reserved: 96895.21484375 | max reserved: 96895.21484375
Number of parameters in transformer layers in billions:  0.97
Number of parameters in embedding layers in billions: 0.53
Total number of parameters in billions: 1.50
Number of parameters in most loaded shard in billions: 0.7493
Activation memory footprint per transformer layer: 136.0 MB
Theoretical memory footprints: weight and optimizer=6431.08 MB, activation=3198.02 MB, total=9629.10 MB

[Rank 0] (after 10 iterations) memory (MB) | allocated: 7450.5960693359375 | max allocated: 13210.283569335938 | reserved: 96895.21484375 | max reserved: 96895.21484375
 [2025-01-07 07:24:31] iteration       10/  937500 | actual seqlen:  4096 | consumed samples:        20480 | elapsed time per iteration (ms): 116772.1 | samples per second: 17.538 | tokens per second: 71837.416 | throughput per GPU (TFLOP/s/GPU): 81.0 | learning rate: 5.000000E-07 | global batch size:  2048 | lm loss: 1.215183E+01 | loss scale: 1.0 | grad norm: 8.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 07:43:34] iteration       20/  937500 | actual seqlen:  4096 | consumed samples:        40960 | elapsed time per iteration (ms): 114316.1 | samples per second: 17.915 | tokens per second: 73380.814 | throughput per GPU (TFLOP/s/GPU): 82.8 | learning rate: 1.000000E-06 | global batch size:  2048 | lm loss: 1.204773E+01 | loss scale: 1.0 | grad norm: 8.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 08:02:50] iteration       30/  937500 | actual seqlen:  4096 | consumed samples:        61440 | elapsed time per iteration (ms): 115548.3 | samples per second: 17.724 | tokens per second: 72598.272 | throughput per GPU (TFLOP/s/GPU): 81.9 | learning rate: 1.500000E-06 | global batch size:  2048 | lm loss: 1.150260E+01 | loss scale: 1.0 | grad norm: 12.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 08:22:07] iteration       40/  937500 | actual seqlen:  4096 | consumed samples:        81920 | elapsed time per iteration (ms): 115726.2 | samples per second: 17.697 | tokens per second: 72486.710 | throughput per GPU (TFLOP/s/GPU): 81.8 | learning rate: 2.000000E-06 | global batch size:  2048 | lm loss: 1.059044E+01 | loss scale: 1.0 | grad norm: 7.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 08:41:18] iteration       50/  937500 | actual seqlen:  4096 | consumed samples:       102400 | elapsed time per iteration (ms): 115105.8 | samples per second: 17.792 | tokens per second: 72877.354 | throughput per GPU (TFLOP/s/GPU): 82.2 | learning rate: 2.500000E-06 | global batch size:  2048 | lm loss: 1.006098E+01 | loss scale: 1.0 | grad norm: 4.098 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 09:00:27] iteration       60/  937500 | actual seqlen:  4096 | consumed samples:       122880 | elapsed time per iteration (ms): 114882.5 | samples per second: 17.827 | tokens per second: 73019.028 | throughput per GPU (TFLOP/s/GPU): 82.4 | learning rate: 3.000000E-06 | global batch size:  2048 | lm loss: 9.756934E+00 | loss scale: 1.0 | grad norm: 2.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 1 with PID 0 on node idc708074 exited on signal 9 (Killed).
--------------------------------------------------------------------------
nohup: ignoring input
+ LAUNCHER_TYPE=mpirun
+ DATA_DIR=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged
+ DATA_CACHE_DIR=
+ DATA_FILE_PREFIX=tokenized_text_document
+ TOKENIZER_MODEL=/lkk/Llama-3.1-8B/original/tokenizer.model
+ TRANSFORMER_IMPL=transformer_engine
+ NUM_NODES=1
+ DP=2
+ TP=1
+ PP=1
+ MICRO_BATCH_SIZE=1
+ EXIT_INTERVAL=0
+ OUTPUT_DIR=
+ OUTPUT_DIR_PREFIX=.
+ CHECKPOINT_SAVE=1
+ SAVE_INTERVAL=2000
+ DIST_CKPT_FORMAT=torch_dist
+ USE_DISTRIBUTED_OPTIMIZER=1
+ USE_DIST_CKPT=0
+ LOAD_DIR=
+ CHECKPOINTS_DIR=
+ VERIFY_CKPT=1
+ TENSORBOARD_DIR=
+ KILL_SWITCH_FILE=
+ HOSTSFILE=
+ CKP_ACT=2
+ RECOMPUTE_NUM_LAYERS=1
+ LOG_INTERVAL=10
+ LLAMA_VER=3.2
+ LLAMA_MODEL_SIZE=1
+ DEVICES_PER_NODE=8
+ SEQ_PARALLEL=1
+ OPTIMIZER=fusedadamw
+ DROPOUT=0.0
+ EVAL_ITERS=100
+ EVAL_INTERVAL=1000
+ USE_FUSED_SDPA=1
+ USE_FUSED_SDPA_WITH_RECOMPUTE=0
+ USE_FAST_SOFTMAX=1
+ USE_FUSED_RMSNORM=1
+ PROFILE_TYPE=
+ PROFILE_STEP_START=3
+ PROFILE_STEP_END=4
+ PROFILE_RANKS=0
+ REDIRECT_LOGS=0
+ DETERMINISTIC_MODE=1
+ FP8=0
+ FP8_FORMAT=hybrid
+ FP8_MARGIN=0
+ FP8_AMAX_COMPUTE_ALGO=max
+ USE_TORCH_COMPILE=0
+ USE_LAZY_MODE=1
+ SKIP_TRAIN=0
+ NUM_WORKERS=2
+ FP8_COVERAGE='mlp_row_parallel=False attention=False'
+ [[ -z '' ]]
+++ dirname /lkk/Megatron-LM/examples/llama/pretrain_llama.sh
++ realpath /lkk/Megatron-LM/examples/llama/../../
+ MEGATRON_LM_ROOT=/lkk/Megatron-LM
+ [[ 8 -ne 2 ]]
+ echo 'NUM_NODES*DEVICES_PER_NODE != DP*TP*PP'
NUM_NODES*DEVICES_PER_NODE != DP*TP*PP
+ exit 1
nohup: ignoring input
+ LAUNCHER_TYPE=mpirun
+ DATA_DIR=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged
+ DATA_CACHE_DIR=
+ DATA_FILE_PREFIX=tokenized_text_document
+ TOKENIZER_MODEL=/lkk/Llama-3.1-8B/original/tokenizer.model
+ TRANSFORMER_IMPL=transformer_engine
+ NUM_NODES=1
+ DP=2
+ TP=1
+ PP=1
+ MICRO_BATCH_SIZE=1
+ EXIT_INTERVAL=0
+ OUTPUT_DIR=
+ OUTPUT_DIR_PREFIX=.
+ CHECKPOINT_SAVE=1
+ SAVE_INTERVAL=2000
+ DIST_CKPT_FORMAT=torch_dist
+ USE_DISTRIBUTED_OPTIMIZER=1
+ USE_DIST_CKPT=0
+ LOAD_DIR=
+ CHECKPOINTS_DIR=
+ VERIFY_CKPT=1
+ TENSORBOARD_DIR=
+ KILL_SWITCH_FILE=
+ HOSTSFILE=
+ CKP_ACT=2
+ RECOMPUTE_NUM_LAYERS=1
+ LOG_INTERVAL=10
+ LLAMA_VER=3.2
+ LLAMA_MODEL_SIZE=1
+ DEVICES_PER_NODE=8
+ SEQ_PARALLEL=1
+ OPTIMIZER=fusedadamw
+ DROPOUT=0.0
+ EVAL_ITERS=100
+ EVAL_INTERVAL=1000
+ USE_FUSED_SDPA=1
+ USE_FUSED_SDPA_WITH_RECOMPUTE=0
+ USE_FAST_SOFTMAX=1
+ USE_FUSED_RMSNORM=1
+ PROFILE_TYPE=
+ PROFILE_STEP_START=3
+ PROFILE_STEP_END=4
+ PROFILE_RANKS=0
+ REDIRECT_LOGS=0
+ DETERMINISTIC_MODE=1
+ FP8=0
+ FP8_FORMAT=hybrid
+ FP8_MARGIN=0
+ FP8_AMAX_COMPUTE_ALGO=max
+ USE_TORCH_COMPILE=0
+ USE_LAZY_MODE=1
+ SKIP_TRAIN=0
+ NUM_WORKERS=2
+ FP8_COVERAGE='mlp_row_parallel=False attention=False'
+ [[ -z '' ]]
+++ dirname /lkk/Megatron-LM/examples/llama/pretrain_llama.sh
++ realpath /lkk/Megatron-LM/examples/llama/../../
+ MEGATRON_LM_ROOT=/lkk/Megatron-LM
+ [[ 8 -ne 2 ]]
+ echo 'NUM_NODES*DEVICES_PER_NODE != DP*TP*PP'
NUM_NODES*DEVICES_PER_NODE != DP*TP*PP
+ exit 1
nohup: ignoring input
+ LAUNCHER_TYPE=mpirun
+ DATA_DIR=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged
+ DATA_CACHE_DIR=
+ DATA_FILE_PREFIX=tokenized_text_document
+ TOKENIZER_MODEL=/lkk/Llama-3.1-8B/original/tokenizer.model
+ TRANSFORMER_IMPL=transformer_engine
+ NUM_NODES=1
+ DP=2
+ TP=1
+ PP=1
+ MICRO_BATCH_SIZE=1
+ EXIT_INTERVAL=0
+ OUTPUT_DIR=
+ OUTPUT_DIR_PREFIX=.
+ CHECKPOINT_SAVE=1
+ SAVE_INTERVAL=2000
+ DIST_CKPT_FORMAT=torch_dist
+ USE_DISTRIBUTED_OPTIMIZER=1
+ USE_DIST_CKPT=0
+ LOAD_DIR=
+ CHECKPOINTS_DIR=
+ VERIFY_CKPT=1
+ TENSORBOARD_DIR=
+ KILL_SWITCH_FILE=
+ HOSTSFILE=
+ CKP_ACT=2
+ RECOMPUTE_NUM_LAYERS=1
+ LOG_INTERVAL=10
+ LLAMA_VER=3.2
+ LLAMA_MODEL_SIZE=1
+ DEVICES_PER_NODE=2
+ SEQ_PARALLEL=1
+ OPTIMIZER=fusedadamw
+ DROPOUT=0.0
+ EVAL_ITERS=100
+ EVAL_INTERVAL=1000
+ USE_FUSED_SDPA=1
+ USE_FUSED_SDPA_WITH_RECOMPUTE=0
+ USE_FAST_SOFTMAX=1
+ USE_FUSED_RMSNORM=1
+ PROFILE_TYPE=
+ PROFILE_STEP_START=3
+ PROFILE_STEP_END=4
+ PROFILE_RANKS=0
+ REDIRECT_LOGS=0
+ DETERMINISTIC_MODE=1
+ FP8=0
+ FP8_FORMAT=hybrid
+ FP8_MARGIN=0
+ FP8_AMAX_COMPUTE_ALGO=max
+ USE_TORCH_COMPILE=0
+ USE_LAZY_MODE=1
+ SKIP_TRAIN=0
+ NUM_WORKERS=2
+ FP8_COVERAGE='mlp_row_parallel=False attention=False'
+ [[ -z '' ]]
+++ dirname /lkk/Megatron-LM/examples/llama/pretrain_llama.sh
++ realpath /lkk/Megatron-LM/examples/llama/../../
+ MEGATRON_LM_ROOT=/lkk/Megatron-LM
+ [[ 2 -ne 2 ]]
+ [[ transformer_engine = \l\o\c\a\l ]]
+ [[ 1 = 1 ]]
+ [[ 0 = 1 ]]
+ [[ 3.2 = \1 ]]
+ [[ 3.2 = \2 ]]
+ [[ 3.2 = \3\.\1 ]]
+ [[ 3.2 = \3\.\2 ]]
+ TOKENIZER_TYPE=Llama3Tokenizer
+ GLOBAL_BATCH_SIZE=2048
+ MAX_SEQ_LEN=4096
+ TRAIN_ITERS=937500
+ ADAM_EPS=1e-5
+ LR_WARMUP_ITERS=8000
+ ROTARY_BASE=500000
+ [[ 1 = \1 ]]
+ HIDDEN_SIZE=2048
+ NUM_HEADS=32
+ NUM_QUERY_GROUPS=8
+ NUM_LAYERS=16
+ FFN_HIDDEN_SIZE=8192
+ LR=4e-4
+ MIN_LR=3e-6
+ [[ 0 -ne 0 ]]
+ SRC_PATH=/lkk/Megatron-LM/pretrain_gpt.py
+ DATA_PATH=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document
+ [[ -z /lkk/Llama-3.1-8B/original/tokenizer.model ]]
+ NUM_DEVICES=2
++ date +%Y%m%d_%H%M
+ RUNTIME=20250107_0904
+ [[ -z '' ]]
+ EXP_NAME=default
+ [[ -z '' ]]
+ data_type=bf16
+ [[ 0 -eq 1 ]]
+ OUTPUT_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904
+ [[ -z '' ]]
+ CHECKPOINTS_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints
+ [[ -z '' ]]
+ LOAD_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints
+ [[ -z '' ]]
+ TENSORBOARD_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard
+ [[ 1 -ne 1 ]]
+ PT_HPU_GPU_MIGRATION=1
+ PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1
+ CMD=
+ [[ mpirun = \m\p\i\r\u\n ]]
+ CMD=' mpirun'
+ CMD=' mpirun --allow-run-as-root'
+ CMD=' mpirun --allow-run-as-root -n 2'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1'
+ [[ 1 -ne 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345'
+ '[' 1 = 0 ']'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2     '
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel'
+ [[ 1 = \1 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader'
+ [[ 0 -eq 1 ]]
+ [[ 2 -eq 1 ]]
+ [[ 2 -eq 2 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective'
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer'
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode'
+ [[ transformer_engine = \t\r\a\n\s\f\o\r\m\e\r\_\e\n\g\i\n\e ]]
+ [[ 0 -eq 1 ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --save-interval 2000'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist'
+ [[ 0 -eq 1 ]]
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA'
+ [[ Llama3Tokenizer = \G\P\T\S\e\n\t\e\n\c\e\P\i\e\c\e\T\o\k\e\n\i\z\e\r ]]
+ [[ Llama3Tokenizer = \L\l\a\m\a\3\T\o\k\e\n\i\z\e\r ]]
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer'
+ CMD=' mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 1     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 4096     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 4096     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 2048     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer --tokenizer-model /lkk/Llama-3.1-8B/original/tokenizer.model'
+ [[ -n '' ]]
+ [[ 0 -eq 1 ]]
+ mpirun --allow-run-as-root -n 2 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345 python /lkk/Megatron-LM/pretrain_gpt.py --transformer-impl transformer_engine --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --distributed-backend nccl --seq-length 4096 --num-layers 16 --hidden-size 2048 --num-attention-heads 32 --group-query-attention --num-query-groups 8 --ffn-hidden-size 8192 --position-embedding-type rope --rotary-base 500000 --max-position-embeddings 4096 --normalization RMSNorm --swiglu --untie-embeddings-and-output-weights --attention-dropout 0.0 --hidden-dropout 0.0 --weight-decay 1e-1 --clip-grad 1.0 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-5 --lr 4e-4 --lr-decay-style cosine --lr-warmup-iters 8000 --min-lr 3e-6 --use-torch-compile=0 --use-fused-sdpa-with-recompute 0 --use-fused-sdpa 1 --use-fused-rmsnorm 1 --use-fast-softmax 1 --micro-batch-size 1 --global-batch-size 2048 --train-iters 937500 --log-interval 10 --log-throughput --disable-bias-linear --optimizer fusedadamw --no-gradient-accumulation-fusion --no-masked-softmax-fusion --use-mcore-models --bf16 --exit-interval 0 --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard --log-validation-ppl-to-tensorboard --log-batch-size-to-tensorboard --log-timers-to-tensorboard --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --eval-interval 1000 --eval-iters 100 --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document --num-workers 2 --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer --tokenizer-model /lkk/Llama-3.1-8B/original/tokenizer.model
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056374480 KB
------------------------------------------------------------------------------
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
using world size: 2, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama3Tokenizer
WARNING: Please specify --split when using --data-path. Using legacy default value of "969, 30, 1"
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
is_gaudi3()=False
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-05
  add_bias_linear ................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_norm_post_sub_block ....................... False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.0
  attention_per_layer_logging ..................... False
  attention_softmax_in_fp32 ....................... False
  attention_z_loss_coeff .......................... 0.0
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. False
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... ['/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  defer_embedding_wgrad_compute ................... False
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  deprecated_use_mcore_models ..................... True
  deterministic_mode .............................. True
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... True
  encoder_num_layers .............................. 16
  encoder_seq_length .............................. 4096
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 100
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 0
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_amax_reduce ................................. False
  fp8_coverage .................................... {'mlp_row_parallel': True, 'attention': True}
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 2048
  gradient_accumulation_fusion .................... False
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 2048
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kill_switch_file ................................ None
  kv_channels ..................................... 64
  lazy_mpu_init ................................... None
  load ............................................ ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints
  load_strict ..................................... True
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0004
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 8000
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 4096
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-06
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_capacity_bins_alignment ..................... 64
  moe_capacity_bins_exp_base ...................... 1.5
  moe_capacity_bins_num ........................... 0
  moe_capacity_bins_optimize_interval ............. 300
  moe_capacity_bins_optimize_max_group ............ 4
  moe_configured_bins ............................. None
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 8
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... fusedadamw
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  profile_type .................................... None
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 500000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints
  save_interval ................................... 2000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 4096
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  shuffle_each_epoch_separately ................... False
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /lkk/Llama-3.1-8B/original/tokenizer.model
  tokenizer_type .................................. Llama3Tokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 937500
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... True
  use_fast_softmax ................................ True
  use_flash_attn .................................. False
  use_fused_rmsnorm ............................... True
  use_fused_sdpa .................................. True
  use_fused_sdpa_with_recompute ................... False
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_torch_compile ............................... False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  verify_checkpoint ............................... True
  verify_checkpoint_model_type .................... LLAMA
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 2
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of micro-batches to constant 1024
> building Llama3Tokenizer tokenizer ...
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
 > padded vocab (size: 128256) with 0 dummy tokens (new size: 128256)
> initializing torch distributed ...
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/lkk/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/lkk/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.263 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
time to initialize megatron (seconds): 26.845
[after megatron is initialized] datetime: 2025-01-07 09:04:26 
building GPT model ...
TransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, expert_model_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=True, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=16, hidden_size=2048, num_attention_heads=32, num_query_groups=8, ffn_hidden_size=8192, kv_channels=64, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, apply_norm_post_sub_block=False, layernorm_epsilon=1e-05, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f5bedcf0820>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=False, test_mode=False, calculate_per_token_loss=False, attention_z_loss_coeff=0.0, init_method=<function init_method_normal.<locals>.init_ at 0x7f5bbfd8ff40>, output_layer_init_method=<function scaled_init_method_normal.<locals>.init_ at 0x7f5bbf45af80>, init_method_std=0.02, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=True, apply_rope_fusion=True, use_fused_rmsnorm=True, use_fused_sdpa=True, use_fused_sdpa_with_recompute=False, use_fast_softmax=True, recompute_granularity='selective', recompute_method=None, recompute_num_layers=None, distribute_saved_activations=False, fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, fp8_amax_reduce=False, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_pre_softmax=False, moe_grouped_gemm=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='allgather', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_capacity_bins_num=0, moe_capacity_bins_exp_base=1.5, moe_capacity_bins_optimize_interval=300, moe_capacity_bins_optimize_max_group=4, moe_capacity_bins_alignment=64, moe_configured_bins=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, enable_cuda_graph=False)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1498482688
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, use_distributed_optimizer=True, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (1498482688 elements):
	module.decoder.layers.13.self_attention.linear_proj.weight
	module.decoder.layers.4.input_layernorm.weight
	module.decoder.layers.12.input_layernorm.weight
	module.decoder.layers.11.mlp.linear_fc2.weight
	module.decoder.layers.3.mlp.linear_fc2.weight
	module.decoder.layers.10.pre_mlp_layernorm.weight
	module.decoder.layers.10.self_attention.linear_proj.weight
	module.decoder.layers.2.pre_mlp_layernorm.weight
	module.decoder.layers.14.mlp.linear_fc1.weight
	module.decoder.layers.3.mlp.linear_fc1.weight
	module.decoder.layers.2.input_layernorm.weight
	module.decoder.layers.9.input_layernorm.weight
	module.decoder.layers.7.pre_mlp_layernorm.weight
	module.decoder.layers.15.pre_mlp_layernorm.weight
	module.decoder.layers.14.mlp.linear_fc2.weight
	module.decoder.layers.5.mlp.linear_fc2.weight
	module.decoder.layers.7.mlp.linear_fc1.weight
	module.decoder.layers.14.input_layernorm.weight
	module.decoder.layers.12.mlp.linear_fc2.weight
	module.decoder.layers.9.mlp.linear_fc1.weight
	module.decoder.layers.3.self_attention.linear_proj.weight
	module.decoder.layers.1.self_attention.linear_qkv.weight
	module.decoder.layers.6.input_layernorm.weight
	module.decoder.layers.15.mlp.linear_fc1.weight
	module.decoder.layers.8.mlp.linear_fc1.weight
	module.decoder.layers.0.input_layernorm.weight
	module.decoder.layers.13.mlp.linear_fc1.weight
	module.decoder.layers.8.mlp.linear_fc2.weight
	module.decoder.layers.4.pre_mlp_layernorm.weight
	module.decoder.layers.4.self_attention.linear_proj.weight
	module.decoder.layers.2.self_attention.linear_qkv.weight
	module.decoder.layers.12.pre_mlp_layernorm.weight
	module.decoder.layers.9.mlp.linear_fc2.weight
	module.decoder.layers.14.self_attention.linear_qkv.weight
	module.decoder.layers.11.input_layernorm.weight
	module.decoder.layers.7.self_attention.linear_proj.weight
	module.decoder.layers.5.self_attention.linear_qkv.weight
	module.decoder.layers.4.self_attention.linear_qkv.weight
	module.decoder.layers.11.self_attention.linear_proj.weight
	module.decoder.layers.6.self_attention.linear_qkv.weight
	module.decoder.layers.3.input_layernorm.weight
	module.decoder.final_layernorm.weight
	module.decoder.layers.1.mlp.linear_fc2.weight
	module.decoder.layers.9.pre_mlp_layernorm.weight
	module.decoder.layers.7.self_attention.linear_qkv.weight
	module.decoder.layers.0.mlp.linear_fc1.weight
	module.decoder.layers.15.mlp.linear_fc2.weight
	module.decoder.layers.9.self_attention.linear_qkv.weight
	module.decoder.layers.8.input_layernorm.weight
	module.decoder.layers.2.mlp.linear_fc2.weight
	module.decoder.layers.12.self_attention.linear_qkv.weight
	module.decoder.layers.14.pre_mlp_layernorm.weight
	module.decoder.layers.12.mlp.linear_fc1.weight
	module.decoder.layers.1.self_attention.linear_proj.weight
	module.decoder.layers.13.mlp.linear_fc2.weight
	module.decoder.layers.10.mlp.linear_fc1.weight
	module.decoder.layers.6.mlp.linear_fc2.weight
	module.decoder.layers.6.pre_mlp_layernorm.weight
	module.decoder.layers.2.mlp.linear_fc1.weight
	module.embedding.word_embeddings.weight
	module.decoder.layers.10.mlp.linear_fc2.weight
	module.decoder.layers.5.input_layernorm.weight
	module.decoder.layers.2.self_attention.linear_proj.weight
	module.decoder.layers.14.self_attention.linear_proj.weight
	module.decoder.layers.13.input_layernorm.weight
	module.decoder.layers.5.self_attention.linear_proj.weight
	module.decoder.layers.0.mlp.linear_fc2.weight
	module.decoder.layers.0.self_attention.linear_qkv.weight
	module.decoder.layers.11.pre_mlp_layernorm.weight
	module.decoder.layers.15.self_attention.linear_qkv.weight
	module.decoder.layers.6.self_attention.linear_proj.weight
	module.decoder.layers.3.pre_mlp_layernorm.weight
	module.decoder.layers.8.self_attention.linear_proj.weight
	module.decoder.layers.13.self_attention.linear_qkv.weight
	module.decoder.layers.12.self_attention.linear_proj.weight
	module.decoder.layers.1.mlp.linear_fc1.weight
	module.decoder.layers.0.pre_mlp_layernorm.weight
	module.decoder.layers.11.mlp.linear_fc1.weight
	module.decoder.layers.10.input_layernorm.weight
	module.decoder.layers.9.self_attention.linear_proj.weight
	module.output_layer.weight
	module.decoder.layers.11.self_attention.linear_qkv.weight
	module.decoder.layers.8.pre_mlp_layernorm.weight
	module.decoder.layers.4.mlp.linear_fc1.weight
	module.decoder.layers.10.self_attention.linear_qkv.weight
	module.decoder.layers.8.self_attention.linear_qkv.weight
	module.decoder.layers.1.pre_mlp_layernorm.weight
	module.decoder.layers.0.self_attention.linear_proj.weight
	module.decoder.layers.15.input_layernorm.weight
	module.decoder.layers.3.self_attention.linear_qkv.weight
	module.decoder.layers.1.input_layernorm.weight
	module.decoder.layers.7.input_layernorm.weight
	module.decoder.layers.6.mlp.linear_fc1.weight
	module.decoder.layers.4.mlp.linear_fc2.weight
	module.decoder.layers.15.self_attention.linear_proj.weight
	module.decoder.layers.5.pre_mlp_layernorm.weight
	module.decoder.layers.13.pre_mlp_layernorm.weight
	module.decoder.layers.7.mlp.linear_fc2.weight
	module.decoder.layers.5.mlp.linear_fc1.weight
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='fusedadamw', lr=0.0004, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f5bec418a00>)
> learning rate decay style: cosine
WARNING: could not find the metadata file ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb2048_mb1_sp1_D2_T1_P1_devices2_20250107_0904/checkpoints/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
(min, max) time across ranks (ms):
    load-checkpoint ................................: (0.79, 57.85)
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-01-07 09:04:27 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1920000000
    validation: 192102400
    test:       204800
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.969), (0.969, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(1920000000, 192102400, 204800), and config=GPTDatasetConfig(random_seed=1234, sequence_length=4096, blend=(['/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document'], None), blend_per_split=[None, None, None], split='969, 30, 1', split_matrix=[(0, 0.969), (0.969, 0.999), (0.999, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer.create_llama3_tokenizer.<locals>._Llama3Tokenizer object at 0x7f5bc01dd660>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None, shuffle_each_epoch_separately=False)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 91269129
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 91269129
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 4fc2c201b05ec5607f1fb0770b6eb38a-GPTDataset-train-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 4fc2c201b05ec5607f1fb0770b6eb38a-GPTDataset-train-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 4fc2c201b05ec5607f1fb0770b6eb38a-GPTDataset-train-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1931285131
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from 7e09de235f166d422f5acc0b10f822a0-GPTDataset-valid-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from 7e09de235f166d422f5acc0b10f822a0-GPTDataset-valid-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from 7e09de235f166d422f5acc0b10f822a0-GPTDataset-valid-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 192511817
INFO:megatron.core.datasets.gpt_dataset:Load the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:	Load the document index from a12f8086c76b4d42f14ed6aefdc69fbf-GPTDataset-test-document_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the sample index from a12f8086c76b4d42f14ed6aefdc69fbf-GPTDataset-test-sample_index.npy
INFO:megatron.core.datasets.gpt_dataset:	Load the shuffle index from a12f8086c76b4d42f14ed6aefdc69fbf-GPTDataset-test-shuffle_index.npy
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 205519
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2025-01-07 09:04:28 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (787.03, 790.99)
    train/valid/test-data-iterators-setup ..........: (347.39, 409.50)
training ...
[before the start of training step] datetime: 2025-01-07 09:04:28 
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
Number of parameters in transformer layers in billions:  0.97
Number of parameters in embedding layers in billions: 0.53
Total number of parameters in billions: 1.50
Number of parameters in most loaded shard in billions: 1.4986
Theoretical memory footprints: weight and optimizer=17149.55 MB
[Rank 0] (after 10 iterations) memory (MB) | allocated: 20228.531616210938 | max allocated: 36768.76989746094 | reserved: 96895.21484375 | max reserved: 96895.21484375
 [2025-01-07 09:51:46] iteration       10/  937500 | actual seqlen:  4096 | consumed samples:        20480 | elapsed time per iteration (ms): 283837.5 | samples per second: 7.215 | tokens per second: 29554.263 | throughput per GPU (TFLOP/s/GPU): 133.4 | learning rate: 5.000000E-07 | global batch size:  2048 | lm loss: 1.214373E+01 | loss scale: 1.0 | grad norm: 8.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
[rank1]: Traceback (most recent call last):
[rank1]:   File "/lkk/Megatron-LM/pretrain_gpt.py", line 271, in <module>
[rank1]:     pretrain(
[rank1]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 309, in pretrain
[rank1]:     iteration, num_floating_point_operations_so_far = train(
[rank1]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 1156, in train
[rank1]:     train_step(forward_step_func,
[rank1]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 618, in train_step
[rank1]:     losses_reduced = forward_backward_func(
[rank1]:   File "/lkk/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 451, in forward_backward_no_pipelining
[rank1]:     output_tensor, num_tokens = forward_step(
[rank1]:   File "/lkk/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 275, in forward_step
[rank1]:     outputs = loss_func(output_tensor)
[rank1]:   File "/lkk/Megatron-LM/pretrain_gpt.py", line 163, in loss_func
[rank1]:     assert not loss[0].isnan(), (
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank1]:     _error_if_any_worker_fails()
[rank1]: RuntimeError: DataLoader worker (pid 37221) is killed by signal: Killed. 
[rank0]: Traceback (most recent call last):
[rank0]:   File "/lkk/Megatron-LM/pretrain_gpt.py", line 271, in <module>
[rank0]:     pretrain(
[rank0]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 309, in pretrain
[rank0]:     iteration, num_floating_point_operations_so_far = train(
[rank0]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 1156, in train
[rank0]:     train_step(forward_step_func,
[rank0]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 618, in train_step
[rank0]:     losses_reduced = forward_backward_func(
[rank0]:   File "/lkk/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 451, in forward_backward_no_pipelining
[rank0]:     output_tensor, num_tokens = forward_step(
[rank0]:   File "/lkk/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 275, in forward_step
[rank0]:     outputs = loss_func(output_tensor)
[rank0]:   File "/lkk/Megatron-LM/pretrain_gpt.py", line 163, in loss_func
[rank0]:     assert not loss[0].isnan(), (
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank0]:     _error_if_any_worker_fails()
[rank0]: RuntimeError: DataLoader worker (pid 37204) is killed by signal: Killed. 
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[17264,1],1]
  Exit code:    1
--------------------------------------------------------------------------
nohup: ignoring input
+ LAUNCHER_TYPE=mpirun
+ DATA_DIR=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged
+ DATA_CACHE_DIR=
+ DATA_FILE_PREFIX=tokenized_text_document
+ TOKENIZER_MODEL=/lkk/Llama-3.1-8B/original/tokenizer.model
+ TRANSFORMER_IMPL=transformer_engine
+ NUM_NODES=1
+ DP=2
+ TP=2
+ PP=1
+ MICRO_BATCH_SIZE=1
+ EXIT_INTERVAL=0
+ OUTPUT_DIR=
+ OUTPUT_DIR_PREFIX=.
+ CHECKPOINT_SAVE=1
+ SAVE_INTERVAL=2000
+ DIST_CKPT_FORMAT=torch_dist
+ USE_DISTRIBUTED_OPTIMIZER=1
+ USE_DIST_CKPT=0
+ LOAD_DIR=
+ CHECKPOINTS_DIR=
+ VERIFY_CKPT=1
+ TENSORBOARD_DIR=
+ KILL_SWITCH_FILE=
+ HOSTSFILE=
+ CKP_ACT=2
+ RECOMPUTE_NUM_LAYERS=1
+ LOG_INTERVAL=10
+ LLAMA_VER=3.2
+ LLAMA_MODEL_SIZE=1
+ DEVICES_PER_NODE=4
+ SEQ_PARALLEL=1
+ OPTIMIZER=fusedadamw
+ DROPOUT=0.0
+ EVAL_ITERS=100
+ EVAL_INTERVAL=1000
+ USE_FUSED_SDPA=1
+ USE_FUSED_SDPA_WITH_RECOMPUTE=0
+ USE_FAST_SOFTMAX=1
+ USE_FUSED_RMSNORM=1
+ PROFILE_TYPE=
+ PROFILE_STEP_START=3
+ PROFILE_STEP_END=4
+ PROFILE_RANKS=0
+ REDIRECT_LOGS=0
+ DETERMINISTIC_MODE=1
+ FP8=0
+ FP8_FORMAT=hybrid
+ FP8_MARGIN=0
+ FP8_AMAX_COMPUTE_ALGO=max
+ USE_TORCH_COMPILE=0
+ USE_LAZY_MODE=1
+ SKIP_TRAIN=0
+ NUM_WORKERS=2
+ FP8_COVERAGE='mlp_row_parallel=False attention=False'
+ [[ -z '' ]]
+++ dirname /lkk/Megatron-LM/examples/llama/pretrain_llama.sh
++ realpath /lkk/Megatron-LM/examples/llama/../../
+ MEGATRON_LM_ROOT=/lkk/Megatron-LM
+ [[ 4 -ne 4 ]]
+ [[ transformer_engine = \l\o\c\a\l ]]
+ [[ 1 = 1 ]]
+ [[ 0 = 1 ]]
+ [[ 3.2 = \1 ]]
+ [[ 3.2 = \2 ]]
+ [[ 3.2 = \3\.\1 ]]
+ [[ 3.2 = \3\.\2 ]]
+ TOKENIZER_TYPE=Llama3Tokenizer
+ GLOBAL_BATCH_SIZE=4096
+ MAX_SEQ_LEN=2048
+ TRAIN_ITERS=937500
+ ADAM_EPS=1e-5
+ LR_WARMUP_ITERS=8000
+ ROTARY_BASE=500000
+ [[ 1 = \1 ]]
+ HIDDEN_SIZE=2048
+ NUM_HEADS=32
+ NUM_QUERY_GROUPS=8
+ NUM_LAYERS=16
+ FFN_HIDDEN_SIZE=8192
+ LR=4e-4
+ MIN_LR=3e-6
+ [[ 0 -ne 0 ]]
+ SRC_PATH=/lkk/Megatron-LM/pretrain_gpt.py
+ DATA_PATH=/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document
+ [[ -z /lkk/Llama-3.1-8B/original/tokenizer.model ]]
+ NUM_DEVICES=4
++ date +%Y%m%d_%H%M
+ RUNTIME=20250107_1023
+ [[ -z '' ]]
+ EXP_NAME=default
+ [[ -z '' ]]
+ data_type=bf16
+ [[ 0 -eq 1 ]]
+ OUTPUT_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023
+ [[ -z '' ]]
+ CHECKPOINTS_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints
+ [[ -z '' ]]
+ LOAD_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints
+ [[ -z '' ]]
+ TENSORBOARD_DIR=./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints
+ mkdir -p ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard
+ [[ 1 -ne 1 ]]
+ PT_HPU_GPU_MIGRATION=1
+ PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1
+ CMD=
+ [[ mpirun = \m\p\i\r\u\n ]]
+ CMD=' mpirun'
+ CMD=' mpirun --allow-run-as-root'
+ CMD=' mpirun --allow-run-as-root -n 4'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1'
+ [[ 1 -ne 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345'
+ '[' 1 = 0 ']'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2     '
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel'
+ [[ 1 = \1 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader'
+ [[ 0 -eq 1 ]]
+ [[ 2 -eq 1 ]]
+ [[ 2 -eq 2 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective'
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer'
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode'
+ [[ transformer_engine = \t\r\a\n\s\f\o\r\m\e\r\_\e\n\g\i\n\e ]]
+ [[ 0 -eq 1 ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --save-interval 2000'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist'
+ [[ 0 -eq 1 ]]
+ [[ 1 -eq 1 ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA'
+ [[ Llama3Tokenizer = \G\P\T\S\e\n\t\e\n\c\e\P\i\e\c\e\T\o\k\e\n\i\z\e\r ]]
+ [[ Llama3Tokenizer = \L\l\a\m\a\3\T\o\k\e\n\i\z\e\r ]]
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer'
+ CMD=' mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345     python /lkk/Megatron-LM/pretrain_gpt.py     --transformer-impl transformer_engine     --tensor-model-parallel-size 2     --pipeline-model-parallel-size 1     --distributed-backend nccl     --seq-length 2048     --num-layers 16     --hidden-size 2048     --num-attention-heads 32     --group-query-attention     --num-query-groups 8     --ffn-hidden-size 8192     --position-embedding-type rope     --rotary-base 500000     --max-position-embeddings 2048     --normalization RMSNorm     --swiglu     --untie-embeddings-and-output-weights     --attention-dropout 0.0     --hidden-dropout 0.0     --weight-decay 1e-1     --clip-grad 1.0     --adam-beta1 0.9     --adam-beta2 0.95     --adam-eps 1e-5     --lr 4e-4     --lr-decay-style cosine     --lr-warmup-iters 8000     --min-lr 3e-6     --use-torch-compile=0     --use-fused-sdpa-with-recompute 0     --use-fused-sdpa 1     --use-fused-rmsnorm 1     --use-fast-softmax 1     --micro-batch-size 1     --global-batch-size 4096     --train-iters 937500     --log-interval 10     --log-throughput     --disable-bias-linear     --optimizer fusedadamw     --no-gradient-accumulation-fusion     --no-masked-softmax-fusion     --use-mcore-models     --bf16     --exit-interval 0     --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard     --log-validation-ppl-to-tensorboard     --log-batch-size-to-tensorboard     --log-timers-to-tensorboard     --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints     --eval-interval 1000     --eval-iters 100     --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document     --num-workers 2      --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer --tokenizer-model /lkk/Llama-3.1-8B/original/tokenizer.model'
+ [[ -n '' ]]
+ [[ 0 -eq 1 ]]
+ mpirun --allow-run-as-root -n 4 --bind-to none -x PT_HPU_GPU_MIGRATION=1 -x PT_TE_ENFORCE_BF16_AMAX_REDUCTION=1 -x MASTER_ADDR=localhost -x MASTER_PORT=12345 python /lkk/Megatron-LM/pretrain_gpt.py --transformer-impl transformer_engine --tensor-model-parallel-size 2 --pipeline-model-parallel-size 1 --distributed-backend nccl --seq-length 2048 --num-layers 16 --hidden-size 2048 --num-attention-heads 32 --group-query-attention --num-query-groups 8 --ffn-hidden-size 8192 --position-embedding-type rope --rotary-base 500000 --max-position-embeddings 2048 --normalization RMSNorm --swiglu --untie-embeddings-and-output-weights --attention-dropout 0.0 --hidden-dropout 0.0 --weight-decay 1e-1 --clip-grad 1.0 --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-5 --lr 4e-4 --lr-decay-style cosine --lr-warmup-iters 8000 --min-lr 3e-6 --use-torch-compile=0 --use-fused-sdpa-with-recompute 0 --use-fused-sdpa 1 --use-fused-rmsnorm 1 --use-fast-softmax 1 --micro-batch-size 1 --global-batch-size 4096 --train-iters 937500 --log-interval 10 --log-throughput --disable-bias-linear --optimizer fusedadamw --no-gradient-accumulation-fusion --no-masked-softmax-fusion --use-mcore-models --bf16 --exit-interval 0 --tensorboard-dir ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard --log-validation-ppl-to-tensorboard --log-batch-size-to-tensorboard --log-timers-to-tensorboard --load ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --eval-interval 1000 --eval-iters 100 --data-path /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document --num-workers 2 --sequence-parallel --no-create-attention-mask-in-dataloader --recompute-granularity selective --use-distributed-optimizer --deterministic-mode --save ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints --save-interval 2000 --dist-ckpt-format torch_dist --verify-checkpoint --verify-checkpoint-model-type LLAMA --tokenizer-type Llama3Tokenizer --tokenizer-model /lkk/Llama-3.1-8B/original/tokenizer.model
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
  return isinstance(object, types.FunctionType)
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/lkk/Megatron-LM/megatron/core/transformer/custom_layers/intel_transformer_engine.py:37: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
============================= HABANA PT BRIDGE CONFIGURATION =========================== 
 PT_HPU_LAZY_MODE = 1
 PT_RECIPE_CACHE_PATH = 
 PT_CACHE_FOLDER_DELETE = 0
 PT_HPU_RECIPE_CACHE_CONFIG = 
 PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
 PT_HPU_LAZY_ACC_PAR_MODE = 1
 PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0
 PT_HPU_EAGER_PIPELINE_ENABLE = 1
 PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1
---------------------------: System Configuration :---------------------------
Num CPU Cores : 160
CPU RAM       : 1056374480 KB
------------------------------------------------------------------------------
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:55: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/encoder_spec.py:49: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/core/models/retro/decoder_spec.py:41: UserWarning: Apex is not installed. Falling back to Torch LayerNorm
  warnings.warn(f'Apex is not installed. Falling back to Torch LayerNorm')
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/lkk/Megatron-LM/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.
  warnings.warn(
/lkk/Megatron-LM/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
using world size: 4, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:Llama3Tokenizer
WARNING: Please specify --split when using --data-path. Using legacy default value of "969, 30, 1"
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
is_gaudi3()=False
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-05
  add_bias_linear ................................. False
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_norm_post_sub_block ....................... False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.0
  attention_per_layer_logging ..................... False
  attention_softmax_in_fp32 ....................... False
  attention_z_loss_coeff .......................... 0.0
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. False
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... ['/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  defer_embedding_wgrad_compute ................... False
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  deprecated_use_mcore_models ..................... True
  deterministic_mode .............................. True
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format ................................ torch_dist
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... True
  encoder_num_layers .............................. 16
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 100
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 0
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_amax_reduce ................................. False
  fp8_coverage .................................... {'mlp_row_parallel': True, 'attention': True}
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 4096
  gradient_accumulation_fusion .................... False
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 2048
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kill_switch_file ................................ None
  kv_channels ..................................... 64
  lazy_mpu_init ................................... None
  load ............................................ ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints
  load_strict ..................................... True
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0004
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 8000
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 3e-06
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_capacity_bins_alignment ..................... 64
  moe_capacity_bins_exp_base ...................... 1.5
  moe_capacity_bins_num ........................... 0
  moe_capacity_bins_optimize_interval ............. 300
  moe_capacity_bins_optimize_max_group ............ 4
  moe_configured_bins ............................. None
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 8
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... fusedadamw
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  profile_type .................................... None
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 500000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints
  save_interval ................................... 2000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... True
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  shuffle_each_epoch_separately ................... False
  skip_train ...................................... False
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. /lkk/Llama-3.1-8B/original/tokenizer.model
  tokenizer_type .................................. Llama3Tokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 937500
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... True
  use_fast_softmax ................................ True
  use_flash_attn .................................. False
  use_fused_rmsnorm ............................... True
  use_fused_sdpa .................................. True
  use_fused_sdpa_with_recompute ................... False
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_torch_compile ............................... False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  verify_checkpoint ............................... True
  verify_checkpoint_model_type .................... LLAMA
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 4
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of micro-batches to constant 2048
> building Llama3Tokenizer tokenizer ...
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
 > padded vocab (size: 128256) with 0 dummy tokens (new size: 128256)
> initializing torch distributed ...
INFO:llama.tokenizer:Reloaded tiktoken model from /lkk/Llama-3.1-8B/original/tokenizer.model
INFO:llama.tokenizer:#words: 128256 - BOS ID: 128000 - EOS ID: 128001
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/lkk/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/lkk/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.219 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
time to initialize megatron (seconds): 64.809
[after megatron is initialized] datetime: 2025-01-07 10:24:00 
building GPT model ...
TransformerConfig(tensor_model_parallel_size=2, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=True, context_parallel_size=1, expert_model_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=True, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=False, async_tensor_model_parallel_allreduce=False, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=16, hidden_size=2048, num_attention_heads=32, num_query_groups=8, ffn_hidden_size=8192, kv_channels=64, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, apply_norm_post_sub_block=False, layernorm_epsilon=1e-05, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f510f91c820>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=False, test_mode=False, calculate_per_token_loss=False, attention_z_loss_coeff=0.0, init_method=<function init_method_normal.<locals>.init_ at 0x7f50e1953f40>, output_layer_init_method=<function scaled_init_method_normal.<locals>.init_ at 0x7f50e1022f80>, init_method_std=0.02, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=False, memory_efficient_layer_norm=False, bias_dropout_fusion=True, apply_rope_fusion=True, use_fused_rmsnorm=True, use_fused_sdpa=True, use_fused_sdpa_with_recompute=False, use_fast_softmax=True, recompute_granularity='selective', recompute_method=None, recompute_num_layers=None, distribute_saved_activations=False, fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, fp8_amax_reduce=False, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_pre_softmax=False, moe_grouped_gemm=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='allgather', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_capacity_bins_num=0, moe_capacity_bins_exp_base=1.5, moe_capacity_bins_optimize_interval=300, moe_capacity_bins_optimize_max_group=4, moe_capacity_bins_alignment=64, moe_configured_bins=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, enable_cuda_graph=False)
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 749275136
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 749275136
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, use_distributed_optimizer=True, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (749275136 elements):
	module.decoder.layers.12.mlp.linear_fc2.weight
	module.decoder.layers.2.pre_mlp_layernorm.weight
	module.decoder.layers.10.pre_mlp_layernorm.weight
	module.decoder.layers.9.input_layernorm.weight
	module.decoder.layers.3.mlp.linear_fc1.weight
	module.decoder.layers.14.mlp.linear_fc2.weight
	module.decoder.layers.7.mlp.linear_fc2.weight
	module.decoder.layers.5.mlp.linear_fc2.weight
	module.decoder.layers.3.self_attention.linear_proj.weight
	module.decoder.layers.15.pre_mlp_layernorm.weight
	module.decoder.layers.9.mlp.linear_fc2.weight
	module.decoder.layers.7.mlp.linear_fc1.weight
	module.decoder.layers.3.self_attention.linear_qkv.weight
	module.decoder.layers.13.self_attention.linear_proj.weight
	module.decoder.layers.7.pre_mlp_layernorm.weight
	module.decoder.layers.6.mlp.linear_fc1.weight
	module.decoder.layers.2.self_attention.linear_proj.weight
	module.decoder.layers.10.self_attention.linear_proj.weight
	module.decoder.layers.6.input_layernorm.weight
	module.decoder.layers.4.self_attention.linear_qkv.weight
	module.decoder.layers.1.mlp.linear_fc2.weight
	module.embedding.word_embeddings.weight
	module.decoder.layers.14.input_layernorm.weight
	module.decoder.layers.11.self_attention.linear_proj.weight
	module.decoder.layers.12.pre_mlp_layernorm.weight
	module.decoder.layers.2.mlp.linear_fc2.weight
	module.decoder.layers.0.self_attention.linear_proj.weight
	module.decoder.layers.8.self_attention.linear_qkv.weight
	module.decoder.layers.4.pre_mlp_layernorm.weight
	module.decoder.layers.12.mlp.linear_fc1.weight
	module.decoder.layers.3.input_layernorm.weight
	module.decoder.layers.1.self_attention.linear_qkv.weight
	module.decoder.layers.13.mlp.linear_fc1.weight
	module.decoder.layers.11.input_layernorm.weight
	module.output_layer.weight
	module.decoder.layers.13.mlp.linear_fc2.weight
	module.decoder.layers.9.pre_mlp_layernorm.weight
	module.decoder.layers.6.mlp.linear_fc2.weight
	module.decoder.layers.4.mlp.linear_fc2.weight
	module.decoder.layers.1.pre_mlp_layernorm.weight
	module.decoder.layers.12.self_attention.linear_proj.weight
	module.decoder.layers.1.input_layernorm.weight
	module.decoder.layers.0.self_attention.linear_qkv.weight
	module.decoder.layers.8.input_layernorm.weight
	module.decoder.layers.15.mlp.linear_fc1.weight
	module.decoder.layers.10.mlp.linear_fc1.weight
	module.decoder.layers.8.mlp.linear_fc1.weight
	module.decoder.layers.6.pre_mlp_layernorm.weight
	module.decoder.layers.1.mlp.linear_fc1.weight
	module.decoder.layers.0.mlp.linear_fc2.weight
	module.decoder.final_layernorm.weight
	module.decoder.layers.14.pre_mlp_layernorm.weight
	module.decoder.layers.12.self_attention.linear_qkv.weight
	module.decoder.layers.5.self_attention.linear_proj.weight
	module.decoder.layers.14.self_attention.linear_qkv.weight
	module.decoder.layers.13.input_layernorm.weight
	module.decoder.layers.11.mlp.linear_fc2.weight
	module.decoder.layers.7.self_attention.linear_qkv.weight
	module.decoder.layers.5.self_attention.linear_qkv.weight
	module.decoder.layers.2.mlp.linear_fc1.weight
	module.decoder.layers.15.self_attention.linear_qkv.weight
	module.decoder.layers.8.self_attention.linear_proj.weight
	module.decoder.layers.6.self_attention.linear_qkv.weight
	module.decoder.layers.5.input_layernorm.weight
	module.decoder.layers.15.self_attention.linear_proj.weight
	module.decoder.layers.3.mlp.linear_fc2.weight
	module.decoder.layers.3.pre_mlp_layernorm.weight
	module.decoder.layers.0.mlp.linear_fc1.weight
	module.decoder.layers.14.self_attention.linear_proj.weight
	module.decoder.layers.11.pre_mlp_layernorm.weight
	module.decoder.layers.10.input_layernorm.weight
	module.decoder.layers.9.mlp.linear_fc1.weight
	module.decoder.layers.4.mlp.linear_fc1.weight
	module.decoder.layers.2.input_layernorm.weight
	module.decoder.layers.0.pre_mlp_layernorm.weight
	module.decoder.layers.15.mlp.linear_fc2.weight
	module.decoder.layers.10.mlp.linear_fc2.weight
	module.decoder.layers.8.mlp.linear_fc2.weight
	module.decoder.layers.8.pre_mlp_layernorm.weight
	module.decoder.layers.2.self_attention.linear_qkv.weight
	module.decoder.layers.7.input_layernorm.weight
	module.decoder.layers.6.self_attention.linear_proj.weight
	module.decoder.layers.5.mlp.linear_fc1.weight
	module.decoder.layers.15.input_layernorm.weight
	module.decoder.layers.14.mlp.linear_fc1.weight
	module.decoder.layers.11.mlp.linear_fc1.weight
	module.decoder.layers.7.self_attention.linear_proj.weight
	module.decoder.layers.0.input_layernorm.weight
	module.decoder.layers.13.pre_mlp_layernorm.weight
	module.decoder.layers.9.self_attention.linear_proj.weight
	module.decoder.layers.13.self_attention.linear_qkv.weight
	module.decoder.layers.5.pre_mlp_layernorm.weight
	module.decoder.layers.10.self_attention.linear_qkv.weight
	module.decoder.layers.4.self_attention.linear_proj.weight
	module.decoder.layers.4.input_layernorm.weight
	module.decoder.layers.1.self_attention.linear_proj.weight
	module.decoder.layers.12.input_layernorm.weight
	module.decoder.layers.11.self_attention.linear_qkv.weight
	module.decoder.layers.9.self_attention.linear_qkv.weight
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='fusedadamw', lr=0.0004, min_lr=3e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=False, overlap_param_gather=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7f510e02cac0>)
> learning rate decay style: cosine
WARNING: could not find the metadata file ./out/llama3.2_1b/bf16_transformer_engine_default_nl16_hs2048_ffn8192_gb4096_mb1_sp1_D2_T2_P1_devices4_20250107_1023/checkpoints/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
(min, max) time across ranks (ms):
    load-checkpoint ................................: (0.57, 49.17)
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-01-07 10:24:01 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      3840000000
    validation: 384204800
    test:       409600
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.969), (0.969, 0.999), (0.999, 1.0)]
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building dataset splits with cls=GPTDataset, sizes=(3840000000, 384204800, 409600), and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document'], None), blend_per_split=[None, None, None], split='969, 30, 1', split_matrix=[(0, 0.969), (0.969, 0.999), (0.999, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer.create_llama3_tokenizer.<locals>._Llama3Tokenizer object at 0x7f50e1d9d780>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None, shuffle_each_epoch_separately=False)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /lkk/Megatron-LM/bigscience/data/oscar/en_tokenized_merged/tokenized_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 91269129
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 91269129
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 3862570262
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 93
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset valid indices
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 385023635
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 306
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset test indices
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 411038
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 10
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2025-01-07 10:50:41 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (685.52, 695.52)
    train/valid/test-data-iterators-setup ..........: (1599210.33, 1600046.88)
training ...
[before the start of training step] datetime: 2025-01-07 10:50:41 
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/mappings.py:182: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/tensor_parallel/layers.py:484: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
/lkk/Megatron-LM/megatron/core/distributed/param_and_grad_buffer.py:135: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  self.communication_handle = torch.distributed._reduce_scatter_base(
Number of parameters in transformer layers in billions:  0.97
Number of parameters in embedding layers in billions: 0.53
Total number of parameters in billions: 1.50
Number of parameters in most loaded shard in billions: 0.7493
Activation memory footprint per transformer layer: 68.0 MB
Theoretical memory footprints: weight and optimizer=8574.77 MB, activation=1599.01 MB, total=10173.78 MB

[Rank 0] (after 10 iterations) memory (MB) | allocated: 9085.281616210938 | max allocated: 12088.377319335938 | reserved: 96895.21484375 | max reserved: 96895.21484375
 [2025-01-07 11:31:11] iteration       10/  937500 | actual seqlen:  2048 | consumed samples:        40960 | elapsed time per iteration (ms): 242949.5 | samples per second: 16.859 | tokens per second: 34528.196 | throughput per GPU (TFLOP/s/GPU): 71.0 | learning rate: 5.000000E-07 | global batch size:  4096 | lm loss: 1.215206E+01 | loss scale: 1.0 | grad norm: 7.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 1] (after 10 iterations) memory (MB) | allocated: 9085.281616210938 | max allocated: 12041.654663085938 | reserved: 96895.21484375 | max reserved: 96895.21484375
 [2025-01-07 12:11:46] iteration       20/  937500 | actual seqlen:  2048 | consumed samples:        81920 | elapsed time per iteration (ms): 243514.5 | samples per second: 16.820 | tokens per second: 34448.091 | throughput per GPU (TFLOP/s/GPU): 70.8 | learning rate: 1.000000E-06 | global batch size:  4096 | lm loss: 1.206429E+01 | loss scale: 1.0 | grad norm: 7.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 12:52:29] iteration       30/  937500 | actual seqlen:  2048 | consumed samples:       122880 | elapsed time per iteration (ms): 244350.8 | samples per second: 16.763 | tokens per second: 34330.181 | throughput per GPU (TFLOP/s/GPU): 70.5 | learning rate: 1.500000E-06 | global batch size:  4096 | lm loss: 1.157250E+01 | loss scale: 1.0 | grad norm: 12.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 13:32:56] iteration       40/  937500 | actual seqlen:  2048 | consumed samples:       163840 | elapsed time per iteration (ms): 242713.0 | samples per second: 16.876 | tokens per second: 34561.835 | throughput per GPU (TFLOP/s/GPU): 71.0 | learning rate: 2.000000E-06 | global batch size:  4096 | lm loss: 1.067640E+01 | loss scale: 1.0 | grad norm: 7.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 14:13:39] iteration       50/  937500 | actual seqlen:  2048 | consumed samples:       204800 | elapsed time per iteration (ms): 244213.2 | samples per second: 16.772 | tokens per second: 34349.521 | throughput per GPU (TFLOP/s/GPU): 70.6 | learning rate: 2.500000E-06 | global batch size:  4096 | lm loss: 1.011544E+01 | loss scale: 1.0 | grad norm: 3.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 14:53:57] iteration       60/  937500 | actual seqlen:  2048 | consumed samples:       245760 | elapsed time per iteration (ms): 241884.3 | samples per second: 16.934 | tokens per second: 34680.254 | throughput per GPU (TFLOP/s/GPU): 71.3 | learning rate: 3.000000E-06 | global batch size:  4096 | lm loss: 9.800391E+00 | loss scale: 1.0 | grad norm: 2.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 15:34:21] iteration       70/  937500 | actual seqlen:  2048 | consumed samples:       286720 | elapsed time per iteration (ms): 242385.4 | samples per second: 16.899 | tokens per second: 34608.552 | throughput per GPU (TFLOP/s/GPU): 71.1 | learning rate: 3.500000E-06 | global batch size:  4096 | lm loss: 9.592865E+00 | loss scale: 1.0 | grad norm: 1.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 16:14:41] iteration       80/  937500 | actual seqlen:  2048 | consumed samples:       327680 | elapsed time per iteration (ms): 241984.4 | samples per second: 16.927 | tokens per second: 34665.903 | throughput per GPU (TFLOP/s/GPU): 71.2 | learning rate: 4.000000E-06 | global batch size:  4096 | lm loss: 9.423639E+00 | loss scale: 1.0 | grad norm: 1.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 16:54:40] iteration       90/  937500 | actual seqlen:  2048 | consumed samples:       368640 | elapsed time per iteration (ms): 239866.7 | samples per second: 17.076 | tokens per second: 34971.964 | throughput per GPU (TFLOP/s/GPU): 71.9 | learning rate: 4.500000E-06 | global batch size:  4096 | lm loss: 9.244732E+00 | loss scale: 1.0 | grad norm: 1.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 17:34:46] iteration      100/  937500 | actual seqlen:  2048 | consumed samples:       409600 | elapsed time per iteration (ms): 240601.9 | samples per second: 17.024 | tokens per second: 34865.095 | throughput per GPU (TFLOP/s/GPU): 71.6 | learning rate: 5.000000E-06 | global batch size:  4096 | lm loss: 9.058987E+00 | loss scale: 1.0 | grad norm: 1.722 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 18:14:50] iteration      110/  937500 | actual seqlen:  2048 | consumed samples:       450560 | elapsed time per iteration (ms): 240402.7 | samples per second: 17.038 | tokens per second: 34893.977 | throughput per GPU (TFLOP/s/GPU): 71.7 | learning rate: 5.500000E-06 | global batch size:  4096 | lm loss: 8.886781E+00 | loss scale: 1.0 | grad norm: 1.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 18:54:34] iteration      120/  937500 | actual seqlen:  2048 | consumed samples:       491520 | elapsed time per iteration (ms): 238385.9 | samples per second: 17.182 | tokens per second: 35189.202 | throughput per GPU (TFLOP/s/GPU): 72.3 | learning rate: 6.000000E-06 | global batch size:  4096 | lm loss: 8.740426E+00 | loss scale: 1.0 | grad norm: 1.506 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 19:35:06] iteration      130/  937500 | actual seqlen:  2048 | consumed samples:       532480 | elapsed time per iteration (ms): 243200.8 | samples per second: 16.842 | tokens per second: 34492.522 | throughput per GPU (TFLOP/s/GPU): 70.9 | learning rate: 6.500000E-06 | global batch size:  4096 | lm loss: 8.609777E+00 | loss scale: 1.0 | grad norm: 3.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 20:15:36] iteration      140/  937500 | actual seqlen:  2048 | consumed samples:       573440 | elapsed time per iteration (ms): 243002.8 | samples per second: 16.856 | tokens per second: 34520.616 | throughput per GPU (TFLOP/s/GPU): 70.9 | learning rate: 7.000000E-06 | global batch size:  4096 | lm loss: 8.512836E+00 | loss scale: 1.0 | grad norm: 1.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 20:55:49] iteration      150/  937500 | actual seqlen:  2048 | consumed samples:       614400 | elapsed time per iteration (ms): 241314.5 | samples per second: 16.974 | tokens per second: 34762.140 | throughput per GPU (TFLOP/s/GPU): 71.4 | learning rate: 7.500000E-06 | global batch size:  4096 | lm loss: 8.424294E+00 | loss scale: 1.0 | grad norm: 2.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 21:35:55] iteration      160/  937500 | actual seqlen:  2048 | consumed samples:       655360 | elapsed time per iteration (ms): 240657.2 | samples per second: 17.020 | tokens per second: 34857.081 | throughput per GPU (TFLOP/s/GPU): 71.6 | learning rate: 8.000000E-06 | global batch size:  4096 | lm loss: 8.342110E+00 | loss scale: 1.0 | grad norm: 2.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 22:16:04] iteration      170/  937500 | actual seqlen:  2048 | consumed samples:       696320 | elapsed time per iteration (ms): 240822.9 | samples per second: 17.008 | tokens per second: 34833.093 | throughput per GPU (TFLOP/s/GPU): 71.6 | learning rate: 8.500000E-06 | global batch size:  4096 | lm loss: 8.255848E+00 | loss scale: 1.0 | grad norm: 1.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 22:56:18] iteration      180/  937500 | actual seqlen:  2048 | consumed samples:       737280 | elapsed time per iteration (ms): 241400.3 | samples per second: 16.968 | tokens per second: 34749.788 | throughput per GPU (TFLOP/s/GPU): 71.4 | learning rate: 9.000000E-06 | global batch size:  4096 | lm loss: 8.170488E+00 | loss scale: 1.0 | grad norm: 1.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-07 23:36:33] iteration      190/  937500 | actual seqlen:  2048 | consumed samples:       778240 | elapsed time per iteration (ms): 241506.3 | samples per second: 16.960 | tokens per second: 34734.536 | throughput per GPU (TFLOP/s/GPU): 71.4 | learning rate: 9.500000E-06 | global batch size:  4096 | lm loss: 8.086997E+00 | loss scale: 1.0 | grad norm: 2.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 00:16:42] iteration      200/  937500 | actual seqlen:  2048 | consumed samples:       819200 | elapsed time per iteration (ms): 240921.4 | samples per second: 17.001 | tokens per second: 34818.861 | throughput per GPU (TFLOP/s/GPU): 71.6 | learning rate: 1.000000E-05 | global batch size:  4096 | lm loss: 8.007687E+00 | loss scale: 1.0 | grad norm: 2.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 00:56:38] iteration      210/  937500 | actual seqlen:  2048 | consumed samples:       860160 | elapsed time per iteration (ms): 239577.3 | samples per second: 17.097 | tokens per second: 35014.199 | throughput per GPU (TFLOP/s/GPU): 72.0 | learning rate: 1.050000E-05 | global batch size:  4096 | lm loss: 7.915990E+00 | loss scale: 1.0 | grad norm: 2.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 01:36:37] iteration      220/  937500 | actual seqlen:  2048 | consumed samples:       901120 | elapsed time per iteration (ms): 239955.5 | samples per second: 17.070 | tokens per second: 34959.013 | throughput per GPU (TFLOP/s/GPU): 71.8 | learning rate: 1.100000E-05 | global batch size:  4096 | lm loss: 7.838631E+00 | loss scale: 1.0 | grad norm: 2.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 02:16:22] iteration      230/  937500 | actual seqlen:  2048 | consumed samples:       942080 | elapsed time per iteration (ms): 238480.7 | samples per second: 17.175 | tokens per second: 35175.213 | throughput per GPU (TFLOP/s/GPU): 72.3 | learning rate: 1.150000E-05 | global batch size:  4096 | lm loss: 7.750446E+00 | loss scale: 1.0 | grad norm: 2.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 02:56:27] iteration      240/  937500 | actual seqlen:  2048 | consumed samples:       983040 | elapsed time per iteration (ms): 240490.4 | samples per second: 17.032 | tokens per second: 34881.260 | throughput per GPU (TFLOP/s/GPU): 71.7 | learning rate: 1.200000E-05 | global batch size:  4096 | lm loss: 7.667408E+00 | loss scale: 1.0 | grad norm: 2.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 03:36:39] iteration      250/  937500 | actual seqlen:  2048 | consumed samples:      1024000 | elapsed time per iteration (ms): 241242.3 | samples per second: 16.979 | tokens per second: 34772.548 | throughput per GPU (TFLOP/s/GPU): 71.5 | learning rate: 1.250000E-05 | global batch size:  4096 | lm loss: 7.594411E+00 | loss scale: 1.0 | grad norm: 2.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 04:16:48] iteration      260/  937500 | actual seqlen:  2048 | consumed samples:      1064960 | elapsed time per iteration (ms): 240895.2 | samples per second: 17.003 | tokens per second: 34822.651 | throughput per GPU (TFLOP/s/GPU): 71.6 | learning rate: 1.300000E-05 | global batch size:  4096 | lm loss: 7.514678E+00 | loss scale: 1.0 | grad norm: 2.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 04:56:48] iteration      270/  937500 | actual seqlen:  2048 | consumed samples:      1105920 | elapsed time per iteration (ms): 239942.0 | samples per second: 17.071 | tokens per second: 34960.982 | throughput per GPU (TFLOP/s/GPU): 71.8 | learning rate: 1.350000E-05 | global batch size:  4096 | lm loss: 7.454062E+00 | loss scale: 1.0 | grad norm: 2.124 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 05:36:36] iteration      280/  937500 | actual seqlen:  2048 | consumed samples:      1146880 | elapsed time per iteration (ms): 238839.8 | samples per second: 17.150 | tokens per second: 35122.318 | throughput per GPU (TFLOP/s/GPU): 72.2 | learning rate: 1.400000E-05 | global batch size:  4096 | lm loss: 7.382987E+00 | loss scale: 1.0 | grad norm: 1.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 06:16:55] iteration      290/  937500 | actual seqlen:  2048 | consumed samples:      1187840 | elapsed time per iteration (ms): 241845.3 | samples per second: 16.936 | tokens per second: 34685.836 | throughput per GPU (TFLOP/s/GPU): 71.3 | learning rate: 1.450000E-05 | global batch size:  4096 | lm loss: 7.315867E+00 | loss scale: 1.0 | grad norm: 1.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 06:57:32] iteration      300/  937500 | actual seqlen:  2048 | consumed samples:      1228800 | elapsed time per iteration (ms): 243706.4 | samples per second: 16.807 | tokens per second: 34420.954 | throughput per GPU (TFLOP/s/GPU): 70.7 | learning rate: 1.500000E-05 | global batch size:  4096 | lm loss: 7.255907E+00 | loss scale: 1.0 | grad norm: 1.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-01-08 07:37:43] iteration      310/  937500 | actual seqlen:  2048 | consumed samples:      1269760 | elapsed time per iteration (ms): 241105.6 | samples per second: 16.988 | tokens per second: 34792.256 | throughput per GPU (TFLOP/s/GPU): 71.5 | learning rate: 1.550000E-05 | global batch size:  4096 | lm loss: 7.200499E+00 | loss scale: 1.0 | grad norm: 1.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
[rank2]: Traceback (most recent call last):
[rank2]:   File "/lkk/Megatron-LM/pretrain_gpt.py", line 271, in <module>
[rank2]:     pretrain(
[rank2]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 309, in pretrain
[rank2]:     iteration, num_floating_point_operations_so_far = train(
[rank2]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 1156, in train
[rank2]:     train_step(forward_step_func,
[rank2]:   File "/lkk/Megatron-LM/megatron/training/training.py", line 618, in train_step
[rank2]:     losses_reduced = forward_backward_func(
[rank2]:   File "/lkk/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 465, in forward_backward_no_pipelining
[rank2]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
[rank2]:   File "/lkk/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 366, in backward_step
[rank2]:     custom_backward(output_tensor[0], output_tensor_grad[0])
[rank2]:   File "/lkk/Megatron-LM/megatron/core/pipeline_parallel/schedules.py", line 156, in custom_backward
[rank2]:     Variable._execution_engine.run_backward(
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank2]:     _error_if_any_worker_fails()
[rank2]: RuntimeError: DataLoader worker (pid 41006) is killed by signal: Killed. 
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
/home/jenkins/workspace/cdsoftwarebuilder/create-binaries-from-sw-sources---bp-dt/repos/hcl/src/platform/gen2_arch_common/signals/manager.cpp::843(DFA): The condition [ false ] failed.  
/home/jenkins/workspace/cdsoftwarebuilder/create-binaries-from-sw-sources---bp-dt/repos/hcl/src/platform/gen2_arch_common/signals/manager.cpp::843(DFA): The condition [ false ] failed.  
/home/jenkins/workspace/cdsoftwarebuilder/create-binaries-from-sw-sources---bp-dt/repos/hcl/src/platform/gen2_arch_common/signals/manager.cpp::843(DFA): The condition [ false ] failed.  
/home/jenkins/workspace/cdsoftwarebuilder/create-binaries-from-sw-sources---bp-dt/repos/hcl/src/platform/gen2_arch_common/signals/manager.cpp::843(DFA): The condition [ false ] failed.  
--------------------------------------------------------------------------
mpirun noticed that process rank 3 with PID 0 on node idc708074 exited on signal 9 (Killed).
--------------------------------------------------------------------------
